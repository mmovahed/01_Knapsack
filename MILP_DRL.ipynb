{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrZG5LHFa0yF5bVtA58k7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmovahed/01_Knapsack/blob/main/MILP_DRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving Mixed Integer Linear Programming (MILP) models using Deep Reinforcement Learning (DRL)"
      ],
      "metadata": {
        "id": "wEKvjhuEfoUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's describe and formulate the simple mixed integer linear programming (MILP) problem for steel blending that we discussed earlier.\n",
        "Problem Description:\n",
        "You want to blend steels with various chemical compositions to obtain 25 tons of steel with a specific chemical composition. The result should have 5% carbon and 5% molybdenum by weight, meaning 25 tons * 5% = 1.25 tons of carbon and 1.25 tons of molybdenum. The objective is to minimize the cost for blending the steel.\n",
        "Variables:\n",
        "- ( $x_i$ ): Binary variables indicating whether you purchase ingot ( $i$ ) (1 if yes, 0 if no).\n",
        "- ( $y_j$ ): Continuous variables representing the quantity in tons of alloy ( $j$ ) that you purchase.\n",
        "- ( $z$ ): Continuous variable representing the quantity of scrap steel that you purchase.\n",
        "Parameters:\n",
        "- ( $w_i$ ): Weight in tons of ingot ( $i$ ).\n",
        "- ( $c_i$ ): Percentage of carbon in ingot ( $i$ ).\n",
        "- ( $m_i$ ): Percentage of molybdenum in ingot ( $i$ ).\n",
        "- ( $p_i$ ): Cost per ton of ingot ( $i$ ).\n",
        "- ( $a_j$ ): Percentage of carbon in alloy ( $j$ ).\n",
        "- ( $b_j$ ): Percentage of molybdenum in alloy ( $j$ ).\n",
        "- ( $q_j$ ): Cost per ton of alloy ( $j$ ).\n",
        "- ( $s_c$ ): Percentage of carbon in scrap steel.\n",
        "- ( $s_m$ ): Percentage of molybdenum in scrap steel.\n",
        "- ( $s_p$ ): Cost per ton of scrap steel.\n",
        "Objective Function:\n",
        "Minimize the total cost of purchasing ingots, alloys, and scrap steel.\n",
        "$$ \\text{Minimize} \\quad Z = \\sum_{i=1}^{4} p_i x_i + \\sum_{j=1}^{3} q_j y_j + s_p z $$\n",
        "\n",
        "Constraints:\n",
        "- The total weight of the blend must be 25 tons.\n",
        "$$ \\sum_{i=1}^{4} w_i x_i + \\sum_{j=1}^{3} y_j + z = 25 $$\n",
        "- The total weight of carbon in the blend must be 1.25 tons.\n",
        "$$ \\sum_{i=1}^{4} \\frac{c_i}{100} w_i x_i + \\sum_{j=1}^{3} \\frac{a_j}{100} y_j + \\frac{s_c}{100} z = 1.25 $$\n",
        "- The total weight of molybdenum in the blend must be 1.25 tons.\n",
        "$$ \\sum_{i=1}^{4} \\frac{m_i}{100} w_i x_i + \\sum_{j=1}^{3} \\frac{b_j}{100} y_j + \\frac{s_m}{100} z = 1.25 $$\n",
        "\n",
        "Bounds:\n",
        "- ( $x_i$ ) are binary variables, so ( $x_i \\in {0, 1}$ ).\n",
        "- ( $y_j$ ) and ( $z$ ) are non-negative.\n",
        "This formulation captures the essence of the steel blending problem where you have to decide on the quantities of different types of steel to purchase in order to meet the desired chemical composition at the minimum cost. The binary variables ( $x_i$ ) ensure that you either purchase an entire ingot or not at all, while the continuous variables ( $y_j$ ) and ( $z$ ) allow for fractional amounts of alloys and scrap steel to be used in the blend."
      ],
      "metadata": {
        "id": "09lhrt5ehZuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "# Define the environment for the MILP problem\n",
        "class MILPEnv:\n",
        "    def __init__(self):\n",
        "        # Define the problem parameters\n",
        "        self.ingots = np.array([[5, 5, 3, 350],\n",
        "                                [3, 4, 3, 330],\n",
        "                                [4, 5, 4, 310],\n",
        "                                [6, 3, 4, 280]])\n",
        "        self.alloys = np.array([[8, 6, 500],\n",
        "                                [7, 7, 450],\n",
        "                                [6, 8, 400]])\n",
        "        self.scrap = np.array([3, 9, 100])\n",
        "        self.target_weight = 25\n",
        "        self.target_composition = np.array([5, 5])  # 5% carbon, 5% molybdenum\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.state = np.zeros(8)  # Initial quantities of ingots and alloys\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Apply the action to the environment\n",
        "        # For simplicity, let's say action is the index to increase the quantity by 1\n",
        "        if action < 4:\n",
        "            self.state[action] += 1  # Buy one more ingot\n",
        "        else:\n",
        "            self.state[action] += 0.1  # Buy 0.1 tons more of alloy or scrap\n",
        "\n",
        "        # Calculate the current weight and composition\n",
        "        weight = np.sum(self.ingots[:, 0] * self.state[:4]) + np.sum(self.alloys[:, 0] * self.state[4:7]) + self.scrap[0] * self.state[7]\n",
        "        composition = np.array([\n",
        "            np.sum(self.ingots[:, 1] * self.state[:4]) + np.sum(self.alloys[:, 1] * self.state[4:7]) + self.scrap[1] * self.state[7],\n",
        "            np.sum(self.ingots[:, 2] * self.state[:4]) + np.sum(self.alloys[:, 2] * self.state[4:7]) + self.scrap[2] * self.state[7]\n",
        "        ])\n",
        "\n",
        "        # Check if the target weight and composition are met\n",
        "        done = weight >= self.target_weight and np.all(composition >= self.target_composition * self.target_weight / 100)\n",
        "        reward = -1 if not done else 1000 - np.sum(self.ingots[:, 3] * self.state[:4]) - np.sum(self.alloys[:, 2] * self.state[4:7]) - self.scrap[2] * self.state[7]\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "# Define the policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        # Define the layers of the network\n",
        "        self.fc = nn.Linear(8, 128)  # 8 inputs for the quantities of ingots and alloys\n",
        "        self.action_head = nn.Linear(128, 8)  # 8 actions corresponding to the quantities\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = torch.relu(self.fc(x))\n",
        "        action_probs = torch.softmax(self.action_head(x), dim=-1)\n",
        "        return action_probs\n",
        "\n",
        "# Initialize the environment and the policy network\n",
        "env = MILPEnv()\n",
        "policy_net = PolicyNetwork()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "max_steps_per_episode = 100\n",
        "total_cost = 0\n",
        "optimal_state = env.reset()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    for t in range(max_steps_per_episode):\n",
        "        # Convert state to PyTorch tensor\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
        "\n",
        "        # Get action probabilities from the policy network\n",
        "        action_probs = policy_net(state_tensor)\n",
        "        m = Categorical(action_probs)\n",
        "\n",
        "        # Sample an action from the distribution\n",
        "        action = m.sample()\n",
        "\n",
        "        # Take the action in the environment\n",
        "        next_state, reward, done = env.step(action.item())\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = -m.log_prob(action) * reward\n",
        "        total_cost += -reward  # Assuming reward is negative cost\n",
        "        # Update the policy network\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        state = next_state\n",
        "        optimal_state = next_state\n",
        "    if episode % 100 == 0:\n",
        "        print(f'Reward in episode {episode} is {reward}')\n",
        "        print(f'Loss in episode {episode} is {loss[0]}')\n",
        "\n",
        "print(f\"Optimal solution found: {optimal_state}\")\n",
        "print(f\"Total cost: {total_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsVmpvWNvFQH",
        "outputId": "ba23c346-088e-45d4-cb67-1f3d11f1e12a"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward in episode 0 is -990.0\n",
            "Loss in episode 0 is -2792.54345703125\n",
            "Reward in episode 100 is -680.0000000000005\n",
            "Loss in episode 100 is -8.106232417048886e-05\n",
            "Reward in episode 200 is -680.0000000000005\n",
            "Loss in episode 200 is -8.106232417048886e-05\n",
            "Reward in episode 300 is -680.0000000000005\n",
            "Loss in episode 300 is -8.106232417048886e-05\n",
            "Reward in episode 400 is -680.0000000000005\n",
            "Loss in episode 400 is -8.106232417048886e-05\n",
            "Reward in episode 500 is -680.0000000000005\n",
            "Loss in episode 500 is -8.106232417048886e-05\n",
            "Reward in episode 600 is -680.0000000000005\n",
            "Loss in episode 600 is -8.106232417048886e-05\n",
            "Reward in episode 700 is -680.0000000000005\n",
            "Loss in episode 700 is -8.106232417048886e-05\n",
            "Reward in episode 800 is -680.0000000000005\n",
            "Loss in episode 800 is -8.106232417048886e-05\n",
            "Reward in episode 900 is -680.0000000000005\n",
            "Loss in episode 900 is -8.106232417048886e-05\n",
            "Optimal solution found: [0.  0.  0.  0.  0.  0.  4.2 0. ]\n",
            "Total cost: 720378.0\n"
          ]
        }
      ]
    }
  ]
}